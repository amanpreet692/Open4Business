
 ### journal_narrative_data.py
**Requirement\:** Download and setup  [GROBID]([https://github.com/kermitt2/grobid-client-python](https://github.com/kermitt2/grobid-client-python)) utility to convert pdf journal articles to xml format.

**Functionality\:**  The entire code for generating the NarrativeArticles Dataset. 
The code is divided into following parts:
1. Get the ISSN of the journals from **data_prime/oa_gold_access.csv** 
2. Based on that info, cal the CoReference API to get the DOI urls of the articles.
3. The CoReference API is called once again to get additional documents based on keywords.
4. From the list of DOI, get the best possible URL to download the article in the pdf format.
5. Then the articles are downloaded to **download_articles** directory.
Uptil here run the script with `--download` parameter

Then convert the pdf articles into xml form using the GROBID utility. (The commands are present on their github page)

6. Now run the script again with `--parse` argument. This parses the XML files generated by GROBID and saves the body and abstract of the article in **source.pkl** and **target.pkl** respectively.


### dataset_utils.py 
This script has utility methods for parsing the GROBID XML files and downloading the pdf of the articles.
**journal_narrative_data.py** calls the methods from this utility.

### preprocess_data.py
`<dataset_name> ('BillSum' or 'Narrative Articles')`

**Functionality\:** Given the complete raw dataset files, this script preprocesses and creates training, validation and test set files which can then be used for fine-tuning the models. All that is handled by the **Dataset** abstract class. Ideally, each new dataset should inherit from this class and implement the **preprocess** and **create_dataset** methods.

The dataset metrics can also be obtained if enabled while running the script.

The data files are written under **<root\>/data/<dataset_name\>** as <train/val/test>.<source/target>.



